{
  "dxt_version": "0.1.0",
  "name": "chrishayuk__mcp-cli",
  "display_name": "mcp-cli",
  "version": "1.0.0",
  "description": "MCP server from chrishayuk/mcp-cli",
  "author": {
    "name": "chrishayuk"
  },
  "server": {
    "type": "python",
    "entry_point": "index.js",
    "mcp_config": {
      "command": "unknown",
      "args": [],
      "env": {}
    }
  },
  "tools": [],
  "prompts": [],
  "keywords": [],
  "user_config": {},
  "readme": "# MCP CLI - Model Context Protocol Command Line Interface\n\nA powerful, feature-rich command-line interface for interacting with Model Context Protocol servers. This client enables seamless communication with LLMs through integration with the [CHUK Tool Processor](https://github.com/chrishayuk/chuk-tool-processor) and [CHUK-LLM](https://github.com/chrishayuk/chuk-llm), providing tool usage, conversation management, and multiple operational modes.\n\n## üîÑ Architecture Overview\n\nThe MCP CLI is built on a modular architecture with clean separation of concerns:\n\n- **[CHUK Tool Processor](https://github.com/chrishayuk/chuk-tool-processor)**: Async-native tool execution and MCP server communication\n- **[CHUK-LLM](https://github.com/chrishayuk/chuk-llm)**: Unified LLM provider configuration and client management\n- **MCP CLI**: Rich user interface and command orchestration (this project)\n\n## üåü Features\n\n### Multiple Operational Modes\n- **Chat Mode**: Conversational interface with streaming responses and automated tool usage\n- **Interactive Mode**: Command-driven shell interface for direct server operations\n- **Command Mode**: Unix-friendly mode for scriptable automation and pipelines\n- **Direct Commands**: Run individual commands without entering interactive mode\n\n### Advanced Chat Interface\n- **Streaming Responses**: Real-time response generation with live UI updates\n- **Concurrent Tool Execution**: Execute multiple tools simultaneously while preserving conversation order\n- **Smart Interruption**: Interrupt streaming responses or tool execution with Ctrl+C\n- **Performance Metrics**: Response timing, words/second, and execution statistics\n- **Rich Formatting**: Markdown rendering, syntax highlighting, and progress indicators\n\n### Comprehensive Provider Support\n- **OpenAI**: GPT models (`gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, etc.)\n- **Anthropic**: Claude models (`claude-3-opus`, `claude-3-sonnet`, `claude-3-haiku`)\n- **Ollama**: Local models (`llama3.2`, `qwen2.5-coder`, `deepseek-coder`, etc.)\n- **Custom Providers**: Extensible architecture for additional providers\n- **Dynamic Switching**: Change providers and models mid-conversation\n\n### Robust Tool System\n- **Automatic Discovery**: Server-provided tools are automatically detected and catalogued\n- **Provider Adaptation**: Tool names are automatically sanitized for provider compatibility\n- **Concurrent Execution**: Multiple tools can run simultaneously with proper coordination\n- **Rich Progress Display**: Real-time progress indicators and execution timing\n- **Tool History**: Complete audit trail of all tool executions\n- **Streaming Tool Calls**: Support for tools that return streaming data\n\n### Advanced Configuration Management\n- **Environment Integration**: API keys and settings via environment variables\n- **File-based Config**: YAML and JSON configuration files\n- **User Preferences**: Persistent settings for active providers and models\n- **Validation & Diagnostics**: Built-in provider health checks and configuration validation\n\n### Enhanced User Experience\n- **Cross-Platform Support**: Windows, macOS, and Linux with platform-specific optimizations\n- **Rich Console Output**: Colorful, formatted output with automatic fallbacks\n- **Command Completion**: Context-aware tab completion for all interfaces\n- **Comprehensive Help**: Detailed help system with examples and usage patterns\n- **Graceful Error Handling**: User-friendly error messages with troubleshooting hints\n\n## üìã Prerequisites\n\n- **Python 3.11 or higher**\n- **API Keys** (as needed):\n  - OpenAI: `OPENAI_API_KEY` environment variable\n  - Anthropic: `ANTHROPIC_API_KEY` environment variable\n  - Custom providers: Provider-specific configuration\n- **Local Services** (as needed):\n  - Ollama: Local installation for Ollama models\n- **MCP Servers**: Server configuration file (default: `server_config.json`)\n\n## üöÄ Installation\n\n### Using UVX\nTo install uxx, use the following instructions:\n\nhttps://docs.astral.sh/uv/getting-started/installation/\n\nOnce installed you can test it works using:\n\n```bash\nuvx mcp-cli --help\n```\n\nor use interactive mode\n\n```bash\nuvx mcp-cli interactive\n```\n\n### Install from Source\n\n1. **Clone the repository**:\n```bash\ngit clone https://github.com/chrishayuk/mcp-cli\ncd mcp-cli  \n```\n\n2. **Install the package**:\n```bash\npip install -e \".\"\n```\n\n3. **Verify installation**:\n```bash\nmcp-cli --help\n```\n\n### Using UV (Recommended)\n\nUV provides faster dependency resolution and better environment management:\n\n```bash\n# Install UV if not already installed\npip install uv\n\n# Install dependencies\nuv sync --reinstall\n\n# Run with UV\nuv run mcp-cli --help\n```\n\n## üß∞ Global Configuration\n\n### Command-line Arguments\n\nGlobal options available for all modes and commands:\n\n- `--server`: Specify server(s) to connect to (comma-separated)\n- `--config-file`: Path to server configuration file (default: `server_config.json`)\n- `--provider`: LLM provider (`openai`, `anthropic`, `ollama`, etc.)\n- `--model`: Specific model to use (provider-dependent)\n- `--disable-filesystem`: Disable filesystem access (default: enabled)\n- `--api-base`: Override API endpoint URL\n- `--api-key`: Override API key\n- `--verbose`: Enable detailed logging\n- `--quiet`: Suppress non-essential output\n\n### Environment Variables\n\n```bash\nexport LLM_PROVIDER=openai              # Default provider\nexport LLM_MODEL=gpt-4o-mini           # Default model\nexport OPENAI_API_KEY=sk-...           # OpenAI API key\nexport ANTHROPIC_API_KEY=sk-ant-...    # Anthropic API key\nexport MCP_TOOL_TIMEOUT=120            # Tool execution timeout (seconds)\n```\n\n## üåê Available Modes\n\n### 1. Chat Mode (Default)\n\nProvides a natural language interface with streaming responses and automatic tool usage:\n\n```bash\n# Default mode (no subcommand needed)\nmcp-cli --server sqlite\n\n# Explicit chat mode\nmcp-cli chat --server sqlite\n\n# With specific provider and model\nmcp-cli chat --server sqlite --provider anthropic --model claude-3-sonnet\n\n# With custom configuration\nmcp-cli chat --server sqlite --provider openai --api-key sk-... --model gpt-4o\n```\n\n### 2. Interactive Mode\n\nCommand-driven shell interface for direct server operations:\n\n```bash\nmcp-cli interactive --server sqlite\n\n# With provider selection\nmcp-cli interactive --server sqlite --provider ollama --model llama3.2\n```\n\n### 3. Command Mode\n\nUnix-friendly interface for automation and scripting:\n\n```bash\n# Process text with LLM\nmcp-cli cmd --server sqlite --prompt \"Analyze this data\" --input data.txt\n\n# Execute tools directly\nmcp-cli cmd --server sqlite --tool list_tables --output tables.json\n\n# Pipeline-friendly processing\necho \"SELECT * FROM users LIMIT 5\" | mcp-cli cmd --server sqlite --tool read_query --input -\n```\n\n### 4. Direct Commands\n\nExecute individual commands without entering interactive mode:\n\n```bash\n# List available tools\nmcp-cli tools --server sqlite\n\n# Show provider configuration\nmcp-cli provider list\n\n# Ping servers\nmcp-cli ping --server sqlite\n\n# List resources\nmcp-cli resources --server sqlite\n```\n\n## ü§ñ Using Chat Mode\n\nChat mode provides the most advanced interface with streaming responses and intelligent tool usage.\n\n### Starting Chat Mode\n\n```bash\n# Simple startup\nmcp-cli --server sqlite\n\n# Multiple servers\nmcp-cli --server sqlite,filesystem\n\n# Specific provider configuration\nmcp-cli --server sqlite --provider anthropic --model claude-3-opus\n```\n\n### Chat Commands (Slash Commands)\n\n#### Provider & Model Management\n```bash\n/provider                           # Show current configuration\n/provider list                      # List all providers\n/provider config                    # Show detailed configuration\n/provider diagnostic               # Test provider connectivity\n/provider set openai api_key sk-... # Configure provider settings\n/provider anthropic                # Switch to Anthropic\n/provider openai gpt-4o            # Switch provider and model\n\n/model                             # Show current model\n/model gpt-4o                      # Switch to specific model\n/models                            # List available models\n```\n\n#### Tool Management\n```bash\n/tools                             # List available tools\n/tools --all                       # Show detailed tool information\n/tools --raw                       # Show raw JSON definitions\n/tools call                        # Interactive tool execution\n\n/toolhistory                       # Show tool execution history\n/th -n 5                          # Last 5 tool calls\n/th 3                             # Details for call #3\n/th --json                        # Full history as JSON\n```\n\n#### Conversation Management\n```bash\n/conversation                      # Show conversation history\n/ch -n 10                         # Last 10 messages\n/ch 5                             # Details for message #5\n/ch --json                        # Full history as JSON\n\n/save conversation.json            # Save conversation to file\n/compact                          # Summarize conversation\n/clear                            # Clear conversation history\n/cls                              # Clear screen only\n```\n\n#### Session Control\n```bash\n/verbose                          # Toggle verbose/compact display\n/interrupt                        # Stop running operations\n/servers                          # List connected servers\n/help                            # Show all commands\n/help tools                       # Help for specific command\n/exit                            # Exit chat mode\n```\n\n### Chat Features\n\n#### Streaming Responses\n- Real-time text generation with live updates\n- Performance metrics (words/second, response time)\n- Graceful interruption with Ctrl+C\n- Progressive markdown rendering\n\n#### Tool Execution\n- Automatic tool discovery and usage\n- Concurrent execution with progress indicators\n- Verbose and compact display modes\n- Complete execution history and timing\n\n#### Provider Integration\n- Seamless switching between providers\n- Model-specific optimizations\n- API key and endpoint management\n- Health monitoring and diagnostics\n\n## üñ•Ô∏è Using Interactive Mode\n\nInteractive mode provides a command shell for direct server interaction.\n\n### Starting Interactive Mode\n\n```bash\nmcp-cli interactive --server sqlite\n```\n\n### Interactive Commands\n\n```bash\nhelp                              # Show available commands\nexit                              # Exit interactive mode\nclear                             # Clear terminal\n\n# Provider management\nprovider                          # Show current provider\nprovider list                     # List providers\nprovider anthropic                # Switch provider\n\n# Tool operations\ntools                             # List tools\ntools --all                       # Detailed tool info\ntools call                        # Interactive tool execution\n\n# Server operations\nservers                           # List servers\nping                              # Ping all servers\nresources                         # List resources\nprompts                           # List prompts\n```\n\n## üìÑ Using Command Mode\n\nCommand mode provides Unix-friendly automation capabilities.\n\n### Command Mode Options\n\n```bash\n--input FILE                      # Input file (- for stdin)\n--output FILE                     # Output file (- for stdout)\n--prompt TEXT                     # Prompt template\n--tool TOOL                       # Execute specific tool\n--tool-args JSON                  # Tool arguments as JSON\n--system-prompt TEXT              # Custom system prompt\n--raw                             # Raw output without formatting\n--single-turn                     # Disable multi-turn conversation\n--max-turns N                     # Maximum conversation turns\n```\n\n### Examples\n\n```bash\n# Text processing\necho \"Analyze this data\" | mcp-cli cmd --server sqlite --input - --output analysis.txt\n\n# Tool execution\nmcp-cli cmd --server sqlite --tool list_tables --raw\n\n# Complex queries\nmcp-cli cmd --server sqlite --tool read_query --tool-args '{\"query\": \"SELECT COUNT(*) FROM users\"}'\n\n# Batch processing with GNU Parallel\nls *.txt | parallel mcp-cli cmd --server sqlite --input {} --output {}.summary --prompt \"Summarize: {{input}}\"\n```\n\n## üîß Provider Configuration\n\n### Automatic Configuration\n\nThe CLI automatically manages provider configurations using the CHUK-LLM library:\n\n```bash\n# Configure a provider\nmcp-cli provider set openai api_key sk-your-key-here\nmcp-cli provider set anthropic api_base https://api.anthropic.com\n\n# Test configuration\nmcp-cli provider diagnostic openai\n\n# List available models\nmcp-cli provider list\n```\n\n### Manual Configuration\n\nThe `chuk_llm` library looks for configuration files in a particular order.\nFirst a file specified by the `CHUK_LLM_CONFIG` environment variable.\nThen, in the current working directory a file like `chuk_llm.yaml`, `providers.yaml`, `llm_config.yaml` or `config/chuk_llm.yaml`.\n\nBut ideally a user wide configuration should be added to `~/.chuk_llm/config.yaml`:\n\n```yaml\nopenai:\n  api_base: https://api.openai.com/v1\n  default_model: gpt-4o-mini\n\nanthropic:\n  api_base: https://api.anthropic.com\n  default_model: claude-3-sonnet\n\nollama:\n  api_base: http://localhost:11434\n  default_model: llama3.2\n```\n\nAPI keys are stored securely in `~/.chuk_llm/.env`:\n\n```bash\nOPENAI_API_KEY=sk-your-key-here\nANTHROPIC_API_KEY=sk-ant-your-key-here\n```\n\n## üìÇ Server Configuration\n\nCreate a `server_config.json` file with your MCP server configurations:\n\n```json\n{\n  \"mcpServers\": {\n    \"sqlite\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"mcp_server.sqlite_server\"],\n      \"env\": {\n        \"DATABASE_PATH\": \"database.db\"\n      }\n    },\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"],\n      \"env\": {}\n    },\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"your-brave-api-key\"\n      }\n    }\n  }\n}\n```\n\n## üìà Advanced Usage Examples\n\n### Multi-Provider Workflow\n\n```bash\n# Start with OpenAI\nmcp-cli chat --server sqlite --provider openai --model gpt-4o\n\n# In chat, switch to Anthropic for reasoning tasks\n> /provider anthropic claude-3-opus\n\n# Switch to Ollama for local processing\n> /provider ollama llama3.2\n\n# Compare responses across providers\n> /provider openai\n> What's the capital of France?\n> /provider anthropic  \n> What's the capital of France?\n```\n\n### Complex Tool Workflows\n\n```bash\n# Database analysis workflow\n> List all tables in the database\n[Tool: list_tables] ‚Üí products, customers, orders\n\n> Show me the schema for the products table\n[Tool: describe_table] ‚Üí id, name, price, category, stock\n\n> Find the top 10 most expensive products\n[Tool: read_query] ‚Üí SELECT name, price FROM products ORDER BY price DESC LIMIT 10\n\n> Export this data to a CSV file\n[Tool: write_file] ‚Üí Saved to expensive_products.csv\n```\n\n### Automation and Scripting\n\n```bash\n# Batch data processing\nfor file in data/*.csv; do\n  mcp-cli cmd --server sqlite \\\n    --tool analyze_data \\\n    --tool-args \"{\\\"file_path\\\": \\\"$file\\\"}\" \\\n    --output \"results/$(basename \"$file\" .csv)_analysis.json\"\ndone\n\n# Pipeline processing\ncat input.txt | \\\n  mcp-cli cmd --server sqlite --prompt \"Extract key entities\" --input - | \\\n  mcp-cli cmd --server sqlite --prompt \"Categorize these entities\" --input - > output.txt\n```\n\n### Performance Monitoring\n\n```bash\n# Enable verbose mode for detailed timing\n> /verbose\n\n# Monitor tool execution times\n> /toolhistory\nTool Call History (15 calls)\n#  | Tool        | Arguments                    | Time\n1  | list_tables | {}                          | 0.12s\n2  | read_query  | {\"query\": \"SELECT...\"}      | 0.45s\n...\n\n# Check provider performance\n> /provider diagnostic\nProvider Diagnostics\nProvider   | Status      | Response Time | Features\nopenai     | ‚úÖ Ready    | 234ms        | üì°üîßüëÅÔ∏è\nanthropic  | ‚úÖ Ready    | 187ms        | üì°üîß\nollama     | ‚úÖ Ready    | 56ms         | üì°üîß\n```\n\n## üîç Troubleshooting\n\n### Common Issues\n\n1. **\"Missing argument 'KWARGS'\" error**:\n   ```bash\n   # Use equals sign format\n   mcp-cli chat --server=sqlite --provider=openai\n   \n   # Or add double dash\n   mcp-cli chat -- --server sqlite --provider openai\n   ```\n\n2. **Provider not found**:\n   ```bash\n   mcp-cli provider diagnostic\n   mcp-cli provider set <provider> api_key <your-key>\n   ```\n\n3. **Tool execution timeout**:\n   ```bash\n   export MCP_TOOL_TIMEOUT=300  # 5 minutes\n   ```\n\n4. **Connection issues**:\n   ```bash\n   mcp-cli ping --server <server-name>\n   mcp-cli servers\n   ```\n\n### Debug Mode\n\nEnable verbose logging for troubleshooting:\n\n```bash\nmcp-cli --verbose chat --server sqlite\nmcp-cli --log-level DEBUG interactive --server sqlite\n```\n\n## üîí Security Considerations\n\n- **API Keys**: Stored securely in environment variables or protected files\n- **File Access**: Filesystem access can be disabled with `--disable-filesystem`\n- **Tool Validation**: All tool calls are validated before execution\n- **Timeout Protection**: Configurable timeouts prevent hanging operations\n- **Server Isolation**: Each server runs in its own process\n\n## üöÄ Performance Features\n\n- **Concurrent Tool Execution**: Multiple tools can run simultaneously\n- **Streaming Responses**: Real-time response generation\n- **Connection Pooling**: Efficient reuse of client connections\n- **Caching**: Tool metadata and provider configurations are cached\n- **Async Architecture**: Non-blocking operations throughout\n\n## üì¶ Dependencies\n\nCore dependencies are organized into feature groups:\n\n- **cli**: Rich terminal UI, command completion, provider integrations\n- **dev**: Development tools, testing utilities, linting\n- **chuk-tool-processor**: Core tool execution and MCP communication\n- **chuk-llm**: Unified LLM provider management\n\nInstall with specific features:\n```bash\npip install \"mcp-cli[cli]\"        # Basic CLI features\npip install \"mcp-cli[cli,dev]\"    # CLI with development tools\n```\n\n## ü§ù Contributing\n\nWe welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.\n\n### Development Setup\n\n```bash\ngit clone https://github.com/chrishayuk/mcp-cli\ncd mcp-cli\npip install -e \".[cli,dev]\"\npre-commit install\n```\n\n### Running Tests\n\n```bash\npytest\npytest --cov=mcp_cli --cov-report=html\n```\n\n## üìú License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## üôè Acknowledgments\n\n- **[CHUK Tool Processor](https://github.com/chrishayuk/chuk-tool-processor)** - Async-native tool execution\n- **[CHUK-LLM](https://github.com/chrishayuk/chuk-llm)** - Unified LLM provider management\n- **[Rich](https://github.com/Textualize/rich)** - Beautiful terminal formatting\n- **[Typer](https://typer.tiangolo.com/)** - CLI framework\n- **[Prompt Toolkit](https://github.com/prompt-toolkit/python-prompt-toolkit)** - Interactive input\n\n## üîó Related Projects\n\n- **[Model Context Protocol](https://modelcontextprotocol.io/)** - Core protocol specification\n- **[MCP Servers](https://github.com/modelcontextprotocol/servers)** - Official MCP server implementations\n- **[CHUK Tool Processor](https://github.com/chrishayuk/chuk-tool-processor)** - Tool execution engine\n- **[CHUK-LLM](https://github.com/chrishayuk/chuk-llm)** - LLM provider abstraction\n",
  "category": "AI Tools",
  "quality_score": 79,
  "archestra_config": {
    "client_config_permutations": null,
    "oauth": {
      "provider": null,
      "required": false
    }
  },
  "github_info": {
    "owner": "chrishayuk",
    "repo": "mcp-cli",
    "url": "https://github.com/chrishayuk/mcp-cli",
    "name": "mcp-cli",
    "path": null,
    "stars": 1599,
    "contributors": 15,
    "issues": 26,
    "releases": false,
    "ci_cd": true,
    "latest_commit_hash": "688698cd83362f96c2e3cf9b3ac56586b6422e33"
  },
  "programming_language": "Python",
  "framework": null,
  "last_scraped_at": "2025-08-03T20:57:46.986Z",
  "evaluation_model": "gemini-2.5-flash",
  "protocol_features": {
    "implementing_tools": true,
    "implementing_prompts": true,
    "implementing_resources": true,
    "implementing_sampling": true,
    "implementing_roots": true,
    "implementing_logging": true,
    "implementing_stdio": true,
    "implementing_streamable_http": false,
    "implementing_oauth2": false
  },
  "dependencies": [
    {
      "name": "asyncio",
      "importance": 8
    },
    {
      "name": "chuk-llm",
      "importance": 10
    },
    {
      "name": "chuk-mcp",
      "importance": 10
    },
    {
      "name": "chuk-tool-processor",
      "importance": 10
    },
    {
      "name": "ibm-watsonx-ai",
      "importance": 7
    },
    {
      "name": "prompt-toolkit",
      "importance": 7
    },
    {
      "name": "python-dotenv",
      "importance": 5
    },
    {
      "name": "rich",
      "importance": 6
    },
    {
      "name": "typer",
      "importance": 10
    }
  ],
  "raw_dependencies": "=== pyproject.toml ===\n[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"mcp-cli\"\nversion = \"0.5.1\"\ndescription = \"A cli for the Model Context Provider\"\nrequires-python = \">=3.11\"\nreadme = \"README.md\"\nauthors = [\n  { name = \"Chris Hay\", email = \"chrishayuk@somejunkmailbox.com\" }\n]\nkeywords = [\"llm\", \"openai\", \"claude\", \"mcp\", \"cli\"]\nlicense = {text = \"MIT\"}\ndependencies = [\n  \"asyncio>=3.4.3\",\n  \"chuk-llm>=0.8.13\",\n  \"chuk-mcp>=0.5\",\n  \"chuk-tool-processor>=0.5.4\",\n  \"ibm-watsonx-ai>=1.3.31\",\n  \"prompt-toolkit>=3.0.50\",\n  \"python-dotenv>=1.0.1\",\n  \"rich>=13.9.4\",\n  \"typer>=0.15.2\",\n]\n\n\n[project.scripts]\nmcp-cli = \"mcp_cli.main:app\"\nmcp-llm = \"mcp_cli.llm.__main__:main\"\n\n[project.optional-dependencies]\nwasm = []\ndev = [\n  \"numpy>=2.2.3\",\n  \"pytest-asyncio>=0.25.3\",\n  \"asyncio>=3.4.3\"\n]\n\n[tool.setuptools]\npackage-dir = { \"\" = \"src\" }\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\ninclude = [\"mcp_cli*\"]\n[dependency-groups]\ndev = [\n  \"colorama>=0.4.6\",\n  \"pydantic>=2.10.2\",\n  \"pytest-asyncio>=0.25.3\",\n]\n"
}
