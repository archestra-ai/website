{
  "name": "multi ai advisor",
  "slug": "yuchenssr__multi-ai-advisor-mcp",
  "description": "council of models for decision",
  "readme": "# Multi-Model Advisor\n## (锵锵四人行)\n\n[![smithery badge](https://smithery.ai/badge/@YuChenSSR/multi-ai-advisor-mcp)](https://smithery.ai/server/@YuChenSSR/multi-ai-advisor-mcp)\n\nA Model Context Protocol (MCP) server that queries multiple Ollama models and combines their responses, providing diverse AI perspectives on a single question. This creates a \"council of advisors\" approach where Claude can synthesize multiple viewpoints alongside its own to provide more comprehensive answers.\n\n<a href=\"https://glama.ai/mcp/servers/@YuChenSSR/multi-ai-advisor-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@YuChenSSR/multi-ai-advisor-mcp/badge\" alt=\"Multi-Model Advisor MCP server\" />\n</a>\n\n```mermaid\ngraph TD\n    A[Start] --> B[Worker Local AI 1 Opinion]\n    A --> C[Worker Local AI 2 Opinion]\n    A --> D[Worker Local AI 3 Opinion]\n    B --> E[Manager AI]\n    C --> E\n    D --> E\n    E --> F[Decision Made]\n```\n\n## Features\n\n- Query multiple Ollama models with a single question\n- Assign different roles/personas to each model\n- View all available Ollama models on your system\n- Customize system prompts for each model\n- Configure via environment variables\n- Integrate seamlessly with Claude for Desktop\n\n## Prerequisites\n\n- Node.js 16.x or higher\n- Ollama installed and running (see [Ollama installation](https://github.com/ollama/ollama#installation))\n- Claude for Desktop (for the complete advisory experience)\n\n## Installation\n\n### Installing via Smithery\n\nTo install multi-ai-advisor-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@YuChenSSR/multi-ai-advisor-mcp):\n\n```bash\nnpx -y @smithery/cli install @YuChenSSR/multi-ai-advisor-mcp --client claude\n```\n\n### Manual Installation\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/YuChenSSR/multi-ai-advisor-mcp.git \n   cd multi-ai-advisor-mcp\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n4. Install required Ollama models:\n   ```bash\n   ollama pull gemma3:1b\n   ollama pull llama3.2:1b\n   ollama pull deepseek-r1:1.5b\n   ```\n\n## Configuration\n\nCreate a `.env` file in the project root with your desired configuration:\n\n```\n# Server configuration\nSERVER_NAME=multi-model-advisor\nSERVER_VERSION=1.0.0\nDEBUG=true\n\n# Ollama configuration\nOLLAMA_API_URL=http://localhost:11434\nDEFAULT_MODELS=gemma3:1b,llama3.2:1b,deepseek-r1:1.5b\n\n# System prompts for each model\nGEMMA_SYSTEM_PROMPT=You are a creative and innovative AI assistant. Think outside the box and offer novel perspectives.\nLLAMA_SYSTEM_PROMPT=You are a supportive and empathetic AI assistant focused on human well-being. Provide considerate and balanced advice.\nDEEPSEEK_SYSTEM_PROMPT=You are a logical and analytical AI assistant. Think step-by-step and explain your reasoning clearly.\n```\n\n## Connect to Claude for Desktop\n\n1. Locate your Claude for Desktop configuration file:\n   - MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Edit the file to add the Multi-Model Advisor MCP server:\n\n```json\n{\n  \"mcpServers\": {\n    \"multi-model-advisor\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/multi-ai-advisor-mcp/build/index.js\"]\n    }\n  }\n}\n```\n\n3. Replace `/absolute/path/to/` with the actual path to your project directory\n\n4. Restart Claude for Desktop\n\n## Usage\n\nOnce connected to Claude for Desktop, you can use the Multi-Model Advisor in several ways:\n\n### List Available Models\n\nYou can see all available models on your system:\n\n```\nShow me which Ollama models are available on my system\n```\n\nThis will display all installed Ollama models and indicate which ones are configured as defaults.\n\n### Basic Usage\n\nSimply ask Claude to use the multi-model advisor:\n\n```\nwhat are the most important skills for success in today's job market, \nyou can use gemma3:1b, llama3.2:1b, deepseek-r1:1.5b to help you \n```\n\nClaude will query all default models and provide a synthesized response based on their different perspectives.\n\n![example](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-24/Q53YEwdTaeTuL6a7.png)\n\n\n\n## How It Works\n\n1. The MCP server exposes two tools:\n   - `list-available-models`: Shows all Ollama models on your system\n   - `query-models`: Queries multiple models with a question\n\n2. When you ask Claude a question referring to the multi-model advisor:\n   - Claude decides to use the `query-models` tool\n   - The server sends your question to multiple Ollama models\n   - Each model responds with its perspective\n   - Claude receives all responses and synthesizes a comprehensive answer\n\n3. Each model can have a different \"persona\" or role assigned, encouraging diverse perspectives.\n\n## Troubleshooting\n\n### Ollama Connection Issues\n\nIf the server can't connect to Ollama:\n- Ensure Ollama is running (`ollama serve`)\n- Check that the OLLAMA_API_URL is correct in your .env file\n- Try accessing http://localhost:11434 in your browser to verify Ollama is responding\n\n### Model Not Found\n\nIf a model is reported as unavailable:\n- Check that you've pulled the model using `ollama pull <model-name>`\n- Verify the exact model name using `ollama list`\n- Use the `list-available-models` tool to see all available models\n\n### Claude Not Showing MCP Tools\n\nIf the tools don't appear in Claude:\n- Ensure you've restarted Claude after updating the configuration\n- Check the absolute path in claude_desktop_config.json is correct\n- Look at Claude's logs for error messages\n\n### RAM is not enough\n\nSome managers' AI models may have chosen larger models, but there is not enough memory to run them. You can try specifying a smaller model (see the [Basic Usage](#basic-usage)) or upgrading the memory.\n\n## License\n\nMIT License\n\nFor more details, please see the LICENSE file in [this project repository](https://github.com/YuChenSSR/multi-ai-advisor-mcp)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.",
  "category": "AI Tools",
  "qualityScore": 59,
  "githubUrl": "https://github.com/YuChenSSR/multi-ai-advisor-mcp",
  "programmingLanguage": "TypeScript",
  "gitHubOrg": "YuChenSSR",
  "gitHubRepo": "multi-ai-advisor-mcp",
  "repositoryPath": null,
  "gh_stars": 59,
  "gh_contributors": 3,
  "gh_issues": 1,
  "gh_releases": false,
  "gh_ci_cd": false,
  "gh_latest_commit_hash": "5ac94979b0fe7b6f0b81e3eb88a88cf8b9326ccb",
  "last_scraped_at": "2025-08-03T20:04:52.678Z",
  "implementing_tools": true,
  "implementing_prompts": true,
  "implementing_resources": false,
  "implementing_sampling": true,
  "implementing_roots": false,
  "implementing_logging": true,
  "implementing_stdio": true,
  "implementing_streamable_http": false,
  "implementing_oauth2": false,
  "rawDependencies": "=== package.json ===\n{\n  \"name\": \"multi-model-advisor\",\n  \"version\": \"1.0.0\",\n  \"description\": \"MCP server that queries multiple Ollama models\",\n  \"main\": \"build/index.js\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"build\": \"tsc && chmod 755 build/index.js\",\n    \"start\": \"node build/index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"mcp\",\n    \"ollama\",\n    \"ai\"\n  ],\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@modelcontextprotocol/sdk\": \"^1.7.0\",\n    \"dotenv\": \"^16.4.7\",\n    \"node-fetch\": \"^3.3.2\",\n    \"zod\": \"^3.24.2\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^22.13.13\",\n    \"@types/node-fetch\": \"^2.6.12\",\n    \"typescript\": \"^5.8.2\"\n  }\n}\n",
  "evaluation_model": "gemini-2.5-flash",
  "configForClients": {
    "mcpServers": {
      "YuChenSSR-multi-ai-advisor-mcp": {
        "command": "node",
        "args": [
          "/absolute/path/to/multi-ai-advisor-mcp/build/index.js"
        ]
      }
    }
  },
  "configForArchestra": {
    "oauth": {},
    "server_config": {
      "args": [
        "/absolute/path/to/multi-ai-advisor-mcp/build/index.js"
      ],
      "command": "node",
      "transport": "stdio",
      "env": {
        "server-basic": "",
        "server-configured": "",
        "server-docker": ""
      }
    }
  },
  "dependencies": [
    {
      "importance": 10,
      "name": "@modelcontextprotocol/sdk"
    },
    {
      "importance": 7,
      "name": "dotenv"
    },
    {
      "importance": 8,
      "name": "node-fetch"
    },
    {
      "importance": 6,
      "name": "zod"
    }
  ]
}